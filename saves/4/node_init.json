{"text_to_wav_instance": {"function_name": "text_to_wav_instance", "import_string": null, "function_string": "\nasync def text_to_wav_instance(node, text):\n    return None\n'''\nimport time\nimport wave\n\nif not is_module_imported(\"TTS\"):\n    from TTS.api import TTS\n    try:\n        tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n    except:\n        pass\ndef split_long_sentence(sentence, max_length=250):\n    if len(sentence) <= max_length:\n        return [sentence]\n    \n    sentences = []\n    while len(sentence) > max_length:\n        # Find the last space before max_length\n        last_space_idx = sentence.rfind(' ', 0, max_length)\n        # If no space is found, split at max_length\n        if last_space_idx == -1:\n            last_space_idx = max_length\n        sentences.append(sentence[:last_space_idx])\n        sentence = sentence[last_space_idx+1:]\n    \n    if sentence:\n        sentences.append(sentence)\n    \n    return sentences\ndef get_wav_duration(file_path):\n    with wave.open(file_path, 'rb') as wav_file:\n        # Get the number of frames in the file\n        frames = wav_file.getnframes()\n        # Get the frame rate (number of frames per second)\n        frame_rate = wav_file.getframerate()\n        # Calculate the duration of the file in seconds\n        duration = frames / float(frame_rate)\n        return duration\nasync def text_to_wav_instance(node, text):\n    filename = \"output.wav\"\n    if tts:\n        if node.trigger_in.startswith(\"prompt\"):\n            #Split audio first\n            # Tokenize the text into sentences\n            sentences = sent_tokenize(text)\n            split_sentences = []\n            for sentence in sentences:\n                if len(sentence) > 250:\n                    split_sentences.extend(split_long_sentence(sentence))\n                else:\n                    split_sentences.append(sentence)\n            node.args[\"sentences\"] = split_sentences\n            \n            \n        print(node.args[\"sentences\"])\n        \n        while not node.args[\"sentences\"][0]:\n            await asyncio.sleep(0.1)\n        \n\n        \n        if node.args[\"sentences\"][0]:\n            # generate speech by cloning a voice using default settings\n            tts.tts_to_file(node.args[\"sentences\"][0],\n            file_path=\"output.wav\",\n            speaker_wav=\"audio.wav\",\n            speed=1,\n            language=\"en\")\n        else:\n            pass\n        \n        try:\n            node.args[\"sentences\"].pop(0)\n        except:\n            pass\n        \n        try:\n            while node.args[\"sound\"].is_playing():\n                await asyncio.sleep(0.1)\n                print(\"Playing\")\n        except Exception as e:\n            print(\"Error, no sound found\", e)\n            node.args[\"sound\"] = None\n        \n        sound = SoundLoader.load(filename)\n        duration = get_wav_duration(\"output.wav\")\n        \n        \n        return {\"speech_wav\" : sound, \"duration\" : duration}\n    else:\n        engine = pyttsx3.init()\n        engine.save_to_file(text, filename)\n        engine.runAndWait()\n        \n        sound = SoundLoader.load(filename)\n        node.args[\"sound\"] = sound\n        return {\"speech_wav\" : sound}\n'''\n        ", "description": null, "documentation": null, "inputs": {"text": "string"}, "outputs": {"speech_wav": "sound", "duration": "num"}}, "play_audio_tts": {"function_name": "play_audio_tts", "import_string": null, "function_string": "\nasync def play_audio_tts(node, sound, duration):\n    if node.trigger_in.startswith(\"text_to_wav_instance\"):\n        if not \"sounds\" in node.args:\n            print(\"Sounds Created\")\n            node.args[\"sounds\"] = []\n            sound.play()\n        if not \"durations\" in node.args:\n            print(\"Durations Created\")\n            node.args[\"durations\"] = []\n        node.args[\"sounds\"].append(sound)\n        node.args[\"durations\"].append(duration)\n        \n    if node.trigger_in.startswith(\"pass_node\"):\n        if node.args[\"sounds\"]:\n            node.args[\"sounds\"][0].play()\n            await asyncio.sleep(node.args[\"durations\"][0])\n            \n            node.args[\"sounds\"].pop(0)\n            node.args[\"durations\"].pop(0)\n            \n            await asyncio.sleep(2)\n            #Delay by audio duration\n        ", "description": null, "documentation": null, "inputs": {"sound": "sound", "duration": "num"}, "outputs": {}}, "stop_audio_tts": {"function_name": "stop_audio_tts", "import_string": null, "function_string": "\nasync def stop_audio_tts(node, sound):\n    pass\n        ", "description": null, "documentation": null, "inputs": {"sound": "sound"}, "outputs": {}}, "file_chooser": {"function_name": "file_chooser", "import_string": null, "function_string": "\nasync def file_chooser(node):\n    print(node, node.node_id, node.output_args)\n    if node.trigger_in.startswith(\"display_output\"):\n        node.output_args = {\"user_image\" : None}\n        return {\"user_image\" : None}\n    else:\n        root = Tk()\n        root.withdraw()\n        file_path = filedialog.askopenfilename()\n        root.destroy()\n        def pop(dt):\n            popup = Popup(title='No file selected',\n                          content=Label(text='No file selected.'),\n                          size_hint=(None, None), size=(400, 200))\n            popup.open()\n        if file_path:\n            #self.image.source = file_path\n            return {\"dir\" : file_path}\n        else:\n            Clock.schedule_once(pop)\n        ", "description": null, "documentation": null, "inputs": {}, "outputs": {"dir": "string"}}, "image_chooser": {"function_name": "image_chooser", "import_string": null, "function_string": "\nasync def image_chooser(node):\n    print(node, node.node_id, node.output_args)\n    if node.trigger_in.startswith(\"display_output\"):\n        node.output_args = {\"user_image\" : None}\n        return {\"user_image\" : None}\n    else:\n        root = Tk()\n        root.withdraw()\n        file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.png;*.jpg;*.jpeg\")])\n        root.destroy()\n        def pop(dt):\n            popup = Popup(title='No file selected',\n                          content=Label(text='No file selected.'),\n                          size_hint=(None, None), size=(400, 200))\n            popup.open()\n        if file_path:\n            #self.image.source = file_path\n            return {\"user_image\" : file_path}\n        else:\n            Clock.schedule_once(pop)\n        ", "description": null, "documentation": null, "inputs": {}, "outputs": {"user_image": "string"}}, "ignition": {"function_name": "ignition", "import_string": null, "function_string": "\nasync def ignition(node):\n    print(\"Ignition\")\n    await asyncio.sleep(.25)\n    return None\n            ", "description": null, "documentation": null, "inputs": {}, "outputs": {}}, "display_output": {"function_name": "display_output", "import_string": null, "function_string": "\nasync def display_output(node, user_input, output, instruct_type, generated_image_path, user_image):\n    app = MDApp.get_running_app()\n    print(\"Display Output: \", user_input, output)\n    user_text = user_input or \"test\"\n    response = output or \"test\"\n    await asyncio.sleep(.25)\n    def update_ui(dt):\n        user_header_text = '[b]User[/b] [size=12][color=#A9A9A9]{}[/color][/size]'.format(app.current_date)\n        bot_header_text = '[b]Bot[/b] [size=12][color=#A9A9A9]{}[/color][/size]'.format(app.current_date)\n        \n        user_message = user_header_text + '\\n' + user_text\n        bot_message = bot_header_text + '\\n' + response\n        \n        user_custom_component = CustomComponent(img_source=\"images/user_logo.png\", txt=user_message)\n        bot_custom_component = CustomComponent(img_source=\"images/bot_logo.png\", txt=bot_message)\n        \n        grid_layout = app.root.get_screen(\"chatbox\").ids.grid_layout\n        \n        grid_layout.add_widget(user_custom_component)\n        print(user_image)\n        if user_image != None:\n            print(user_image)\n            grid_layout.add_widget(CustomImageComponent(img_source=user_image))\n        grid_layout.add_widget(bot_custom_component)\n        \n        if instruct_type == 1:\n            #image_components.append(CustomImageComponent(img_source=generated_image_path))\n            grid_layout.add_widget(CustomImageComponent(img_source=generated_image_path))\n\n    # Schedule the update_ui function to run on the main thread\n    Clock.schedule_once(update_ui)\n        ", "description": null, "documentation": null, "inputs": {"user_input": "string", "output": "string", "instruct_type": "num", "generated_image_path": "string", "user_image": "string"}, "outputs": {}}, "select_model": {"function_name": "select_model", "import_string": null, "function_string": "\nasync def select_model(node):\n    print(\"select_model\")\n    await asyncio.sleep(.25)\n    return None\n            ", "description": null, "documentation": null, "inputs": {}, "outputs": {"model": "string"}}, "user_input": {"function_name": "user_input", "import_string": null, "function_string": "\nasync def user_input(node):\n    print(\"user_input\")\n    await asyncio.sleep(.25)\n    return None\n            ", "description": null, "documentation": null, "inputs": {}, "outputs": {"user_input": "string"}}, "pass_node": {"function_name": "pass_node", "import_string": null, "function_string": "\nasync def pass_node(node):\n    return None\n            ", "description": null, "documentation": null, "inputs": {}, "outputs": {}}, "context": {"function_name": "context", "import_string": null, "function_string": "\nasync def context(node):\n    print(\"context\")\n    await asyncio.sleep(.25)\n    return None\n            ", "description": null, "documentation": null, "inputs": {}, "outputs": {"context": "string"}}, "reset_outputs": {"function_name": "reset_outputs", "import_string": null, "function_string": "\nasync def reset_outputs(node):\n    return None\n            ", "description": null, "documentation": null, "inputs": {}, "outputs": {}}, "prompt": {"function_name": "prompt", "import_string": null, "function_string": "\nasync def prompt(node, model=None, user_prompt=None, context=None):\n    app = MDApp.get_running_app()\n    print(\"Prompt\")\n    print(model, user_prompt, context)\n    await asyncio.sleep(.25)\n    user_text = user_prompt\n    instruct_type = app.get_instruct_type(user_text)\n    if context:\n        context = \"OCR output:\\n\" + context\n        print(\"context: \", context)\n    generated_image_path = \"\"\n    if instruct_type == 1:\n        generated_image_path = app.generate_image_prompt(user_text)\n    if instruct_type == 2:\n        pass\n    # Continue the conversation            \n    response = app.continue_conversation(context=context)\n    print(\"output: \", response)\n    return {\"output\" : response, \"instruct_type\" : instruct_type, \"generated_image_path\" : generated_image_path}\n        ", "description": null, "documentation": null, "inputs": {"model": "string", "user_prompt": "string", "context": "string"}, "outputs": {"output": "string", "instruct_type": "num", "generated_image_path": "string"}}, "image_to_text": {"function_name": "image_to_text", "import_string": null, "function_string": "\nasync def image_to_text(node, image_path=None):\n    if image_path:\n        # Load the image using PIL\n        \n        image = Image.open(image_path)\n\n        # Convert the image to a format OpenCV can work with\n        image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n\n        # Use pytesseract to get detailed OCR results\n        detailed_data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n\n        # Initialize variables to store sentence/paragraph bounding boxes and text\n        boxes = []\n        current_box = None\n        current_text = \"\"\n\n        # Loop over each of the text elements found in the image\n        for i in range(len(detailed_data['level'])):\n            (x, y, w, h) = (detailed_data['left'][i], detailed_data['top'][i], detailed_data['width'][i], detailed_data['height'][i])\n            text = detailed_data['text'][i]\n            conf = int(detailed_data['conf'][i])\n            \n            # Only consider text elements with a confidence above a certain threshold\n            if conf > 40:\n                if current_box is None:\n                    # Start a new bounding box and text group\n                    current_box = (x, y, x + w, y + h)\n                    current_text = text\n                else:\n                    # Check if the text element is on a new line\n                    if y > current_box[3]:\n                        # Add a newline character\n                        current_text += \"\\n\"\n                    # Expand the current bounding box to include the new text element\n                    current_box = (\n                        min(current_box[0], x),\n                        min(current_box[1], y),\n                        max(current_box[2], x + w),\n                        max(current_box[3], y + h)\n                    )\n                    # Append text to the current group\n                    current_text += \" \" + text\n                \n                # Check if the next element is a new paragraph or sentence (using heuristic)\n                if i == len(detailed_data['level']) - 1 or detailed_data['block_num'][i] != detailed_data['block_num'][i + 1]:\n                    boxes.append((current_box, current_text))\n                    current_box = None\n                    current_text = \"\"\n        output_text = \"\"\n        # Draw bounding boxes around sentences/paragraphs and print the text and bounding box coordinates\n        for ((x1, y1, x2, y2), text) in boxes:\n            #cv2.rectangle(image_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            output_text += f\"Text:\\n{text}\\n\\n\"\n        print(output_text)\n        return {\"output_text\" : output_text}\n    else:\n        return {\"output_text\" : None}\n        ", "description": null, "documentation": null, "inputs": {"image_path": "string"}, "outputs": {"output_text": "string"}}, "translate_language": {"function_name": "translate_language", "import_string": null, "function_string": "\nasync def translate_language(node, input_text=None, input_language=None, output_language=\"English\"):\n    if input_text:\n        if input_language != output_language:\n            global cohere_api_key\n            co = cohere.Client(cohere_api_key) # This is your trial API key\n            fix_prompt = f'Translate each sentence into {output_language}, output only the translation:'\n            response = co.generate(\n                model='c4ai-aya-23',\n                prompt=fix_prompt + input_text,\n                max_tokens=20000,\n                temperature=0.9,\n                k=0,\n                stop_sequences=[],\n                return_likelihoods='NONE')\n            print(\"Translated\", response.generations[0].text)\n            output_text = response.generations[0].text\n            return {\"output_text\" : output_text}\n        else:\n            return {\"output_text\" : input_text}\n    else:\n        return {\"output_text\" : None}\n        ", "description": null, "documentation": null, "inputs": {"input_text": "string", "input_language": "string", "output_language": "string"}, "outputs": {"output_text": "string"}}, "detect_language": {"function_name": "detect_language", "import_string": null, "function_string": "\nasync def detect_language(node, input_text=None):\n    if input_text:\n        try:\n            co = cohere.Client(cohere_api_key) # This is your trial API key\n            # Detect the language of the text\n            # Print the language code and name\n            response = co.generate(\n            model='c4ai-aya-23',\n            prompt=\"Input Text:\\n\" + input_text + \"\\nDetect language, output only ISO 639 language code in format, based on language not content:\\ncode: en,fr,jp,etc.\",\n            max_tokens=30,\n            temperature=0.1,\n            k=0,\n            stop_sequences=[],\n            return_likelihoods='NONE')\n            language_code=response.generations[0].text\n            print(response.generations[0].text)\n            language = language_codes[language_code]\n            print(language_code)\n            print(\"language: \", language)\n            return {\"language\" : language}\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return {\"language\" : \"unknown\"}\n    else:\n        return {\"language\" : \"unknown\"}\n        ", "description": null, "documentation": null, "inputs": {"input_text": "string"}, "outputs": {"language": "string"}}, "delay": {"function_name": "delay", "import_string": null, "function_string": "\nasync def delay(node, delay_seconds=None):\n    if node.trigger_in.startswith(\"time_delta_seconds\"):\n        \n        asyncio.sleep(delay_seconds)\n    elif delay_seconds:\n        asyncio.sleep(delay_seconds)\n    return None\n        ", "description": null, "documentation": null, "inputs": {"delay_seconds": "string"}, "outputs": {}}, "time_delta_seconds_from_now": {"function_name": "time_delta_seconds", "import_string": null, "function_string": "\nasync def time_delta_seconds(node, given_date_time_str):\n    now = datetime.now()\n    \n    # Given date and time\n    given_date_time = datetime.strptime(given_date_time_str, \"%Y-%m-%d %H:%M:%S\")\n    \n    # Calculate the difference\n    delta = now - given_date_time\n    \n    # Convert the difference to seconds\n    delta_seconds = delta.total_seconds()\n    \n    return {\"seconds\" : delta_seconds}\n        ", "description": null, "documentation": null, "inputs": {"given_date_time_str": "string"}, "outputs": {"seconds": "num"}}, "decide_output_language": {"function_name": "decide_output_language", "import_string": null, "function_string": "\nasync def decide_output_language(node, user_language=None, listener_language=None, user_prompt=None, user_info=None, listener_info=None):\n    if input_text:\n        try:\n            language_code = detect(user_prompt)\n            language = language_codes[language_code]\n            return {\"language\" : language}\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return {\"language\" : \"English\"}\n    else:\n        return {\"language\" : \"English\"}\n        ", "description": null, "documentation": null, "inputs": {"user_prompt": "string", "user_language": "string", "user_info": "string", "listener_language": "string", "listener_info": "string"}, "outputs": {"language": "string"}}, "Button : camera_icon": {"function_name": "Button : camera_icon", "import_string": null, "function_string": null, "description": null, "documentation": null, "inputs": {}, "outputs": {}}}