{"file_chooser": {"function_name": "file_chooser", "import_string": null, "function_string": "\nasync def file_chooser(node):\n    print(node, node.node_id, node.output_args)\n    if node.trigger_in.startswith(\"display_output\"):\n        node.output_args = {\"user_image\" : None}\n        return {\"user_image\" : None}\n    else:\n        root = Tk()\n        root.withdraw()\n        file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.png;*.jpg;*.jpeg\")])\n        root.destroy()\n        def pop(dt):\n            popup = Popup(title='No file selected',\n                          content=Label(text='No file selected.'),\n                          size_hint=(None, None), size=(400, 200))\n            popup.open()\n        if file_path:\n            #self.image.source = file_path\n            return {\"user_image\" : file_path}\n        else:\n            Clock.schedule_once(pop)\n        ", "description": null, "documentation": null, "inputs": {}, "outputs": {"user_image": "string"}}, "ignition": {"function_name": "ignition", "import_string": null, "function_string": "\nasync def ignition(node):\n    print(\"Ignition\")\n    await asyncio.sleep(.25)\n    return None\n            ", "description": null, "documentation": null, "inputs": {}, "outputs": {}}, "display_output": {"function_name": "display_output", "import_string": null, "function_string": "\nasync def display_output(node, user_input, output, instruct_type, generated_image_path, user_image):\n    app = MDApp.get_running_app()\n    print(\"Display Output: \", user_input, output)\n    user_text = user_input or \"test\"\n    response = output or \"test\"\n    await asyncio.sleep(.25)\n    def update_ui(dt):\n        user_header_text = '[b]User[/b] [size=12][color=#A9A9A9]{}[/color][/size]'.format(app.current_date)\n        bot_header_text = '[b]Bot[/b] [size=12][color=#A9A9A9]{}[/color][/size]'.format(app.current_date)\n        \n        user_message = user_header_text + '\\n' + user_text\n        bot_message = bot_header_text + '\\n' + response\n        \n        user_custom_component = CustomComponent(img_source=\"images/user_logo.png\", txt=user_message)\n        bot_custom_component = CustomComponent(img_source=\"images/bot_logo.png\", txt=bot_message)\n        \n        grid_layout = app.root.get_screen(\"chatbox\").ids.grid_layout\n        \n        grid_layout.add_widget(user_custom_component)\n        print(user_image)\n        if user_image != None:\n            print(user_image)\n            grid_layout.add_widget(CustomImageComponent(img_source=user_image))\n        grid_layout.add_widget(bot_custom_component)\n        \n        if instruct_type == 1:\n            #image_components.append(CustomImageComponent(img_source=generated_image_path))\n            grid_layout.add_widget(CustomImageComponent(img_source=generated_image_path))\n\n    # Schedule the update_ui function to run on the main thread\n    Clock.schedule_once(update_ui)\n        ", "description": null, "documentation": null, "inputs": {"user_input": "string", "output": "string", "instruct_type": "num", "generated_image_path": "string", "user_image": "string"}, "outputs": {}}, "select_model": {"function_name": "select_model", "import_string": null, "function_string": "\nasync def select_model(node):\n    print(\"select_model\")\n    await asyncio.sleep(.25)\n    return None\n            ", "description": null, "documentation": null, "inputs": {}, "outputs": {"model": "string"}}, "user_input": {"function_name": "user_input", "import_string": null, "function_string": "\nasync def user_input(node):\n    print(\"user_input\")\n    await asyncio.sleep(.25)\n    return None\n            ", "description": null, "documentation": null, "inputs": {}, "outputs": {"user_input": "string"}}, "context": {"function_name": "context", "import_string": null, "function_string": "\nasync def context(node):\n    print(\"context\")\n    await asyncio.sleep(.25)\n    return None\n            ", "description": null, "documentation": null, "inputs": {}, "outputs": {"context": "string"}}, "prompt": {"function_name": "prompt", "import_string": null, "function_string": "\nasync def prompt(node, model=None, user_prompt=None, context=None):\n    app = MDApp.get_running_app()\n    print(\"Prompt\")\n    print(model, user_prompt, context)\n    await asyncio.sleep(.25)\n    user_text = user_prompt\n    instruct_type = app.get_instruct_type(user_text)\n    if context:\n        context = \"OCR output:\\n\" + context\n        print(\"context: \", context)\n    generated_image_path = \"\"\n    if instruct_type == 1:\n        generated_image_path = app.generate_image_prompt(user_text)\n    # Continue the conversation            \n    response = app.continue_conversation(context=context)\n    print(\"output: \", response)\n    return {\"output\" : response, \"instruct_type\" : instruct_type, \"generated_image_path\" : generated_image_path}\n        ", "description": null, "documentation": null, "inputs": {"model": "string", "user_prompt": "string", "context": "string"}, "outputs": {"output": "string", "instruct_type": "num", "generated_image_path": "string"}}, "image_to_text": {"function_name": "image_to_text", "import_string": null, "function_string": "\nasync def image_to_text(node, image_path=None):\n    if image_path:\n        # Load the image using PIL\n        \n        image = Image.open(image_path)\n\n        # Convert the image to a format OpenCV can work with\n        image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n\n        # Use pytesseract to get detailed OCR results\n        detailed_data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n\n        # Initialize variables to store sentence/paragraph bounding boxes and text\n        boxes = []\n        current_box = None\n        current_text = \"\"\n\n        # Loop over each of the text elements found in the image\n        for i in range(len(detailed_data['level'])):\n            (x, y, w, h) = (detailed_data['left'][i], detailed_data['top'][i], detailed_data['width'][i], detailed_data['height'][i])\n            text = detailed_data['text'][i]\n            conf = int(detailed_data['conf'][i])\n            \n            # Only consider text elements with a confidence above a certain threshold\n            if conf > 40:\n                if current_box is None:\n                    # Start a new bounding box and text group\n                    current_box = (x, y, x + w, y + h)\n                    current_text = text\n                else:\n                    # Check if the text element is on a new line\n                    if y > current_box[3]:\n                        # Add a newline character\n                        current_text += \"\\n\"\n                    # Expand the current bounding box to include the new text element\n                    current_box = (\n                        min(current_box[0], x),\n                        min(current_box[1], y),\n                        max(current_box[2], x + w),\n                        max(current_box[3], y + h)\n                    )\n                    # Append text to the current group\n                    current_text += \" \" + text\n                \n                # Check if the next element is a new paragraph or sentence (using heuristic)\n                if i == len(detailed_data['level']) - 1 or detailed_data['block_num'][i] != detailed_data['block_num'][i + 1]:\n                    boxes.append((current_box, current_text))\n                    current_box = None\n                    current_text = \"\"\n        output_text = \"\"\n        # Draw bounding boxes around sentences/paragraphs and print the text and bounding box coordinates\n        for ((x1, y1, x2, y2), text) in boxes:\n            #cv2.rectangle(image_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            output_text += f\"Text:\\n{text}\\n\\n\"\n        print(output_text)\n        return {\"output_text\" : output_text}\n    else:\n        return {\"output_text\" : None}\n        ", "description": null, "documentation": null, "inputs": {"image_path": "string"}, "outputs": {"output_text": "string"}}, "translate_language": {"function_name": "translate_language", "import_string": null, "function_string": "\nasync def translate_language(node, input_text=None, input_language=None, output_language=\"English\"):\n    if input_text:\n        if input_language != output_language:\n            global cohere_api_key\n            co = cohere.Client(cohere_api_key) # This is your trial API key\n            fix_prompt = f'Translate each sentence into {output_language}, output only the translation:'\n            response = co.generate(\n                model='c4ai-aya-23',\n                prompt=fix_prompt + input_text,\n                max_tokens=20000,\n                temperature=0.9,\n                k=0,\n                stop_sequences=[],\n                return_likelihoods='NONE')\n            print(\"Translated\", response.generations[0].text)\n            output_text = response.generations[0].text\n            return {\"output_text\" : output_text}\n        else:\n            return {\"output_text\" : input_text}\n    else:\n        return {\"output_text\" : None}\n        ", "description": null, "documentation": null, "inputs": {"input_text": "string", "input_language": "string", "output_language": "string"}, "outputs": {"output_text": "string"}}, "detect_language": {"function_name": "detect_language", "import_string": null, "function_string": "\nasync def detect_language(node, input_text=None):\n    if input_text:\n        try:\n            co = cohere.Client(cohere_api_key) # This is your trial API key\n            # Detect the language of the text\n            # Print the language code and name\n            response = co.generate(\n            model='c4ai-aya-23',\n            prompt=\"Input Text:\\n\" + input_text + \"\\nDetect language, output only ISO 639 language code in format, based on language not content:\\ncode: en,fr,jp,etc.\",\n            max_tokens=30,\n            temperature=0.1,\n            k=0,\n            stop_sequences=[],\n            return_likelihoods='NONE')\n            language_code=response.generations[0].text\n            print(response.generations[0].text)\n            language = language_codes[language_code]\n            print(language_code)\n            print(\"language: \", language)\n            return {\"language\" : language}\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return {\"language\" : \"unknown\"}\n    else:\n        return {\"language\" : \"unknown\"}\n        ", "description": null, "documentation": null, "inputs": {"input_text": "string"}, "outputs": {"language": "string"}}, "decide_output_language": {"function_name": "decide_output_language", "import_string": null, "function_string": "\nasync def decide_output_language(node, user_language=None, listener_language=None, user_prompt=None, user_info=None, listener_info=None):\n    if input_text:\n        try:\n            language_code = detect(user_prompt)\n            language = language_codes[language_code]\n            return {\"language\" : language}\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return {\"language\" : \"English\"}\n    else:\n        return {\"language\" : \"English\"}\n        ", "description": null, "documentation": null, "inputs": {"user_prompt": "string", "user_language": "string", "user_info": "string", "listener_language": "string", "listener_info": "string"}, "outputs": {"language": "string"}}, "Button : camera_icon": {"function_name": "Button : camera_icon", "import_string": null, "function_string": null, "description": null, "documentation": null, "inputs": {}, "outputs": {}}}