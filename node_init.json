{"ignition": {"function_name": "ignition", "import_string": null, "function_string": "\ndef ignition(node):\n    print(\"Ignition\")\n    await asyncio.sleep(.25)\n    return None\n            ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string"}}, "process_connection_queue": {"function_name": "process_connection_queue", "import_string": null, "function_string": "\ndef process_connection_queue(node):\n    if not connection_queue.empty():\n        user_id, sid = connection_queue.get()  # Get the next connection request\n        if user_id:\n            clients[user_id] = sid  # Register the client\n            print(f\"Client {user_id} connected with session ID {sid}\")\n            node.args[\"user_id\"] = user_id\n            return {\"user_id\" : user_id}\n        else:\n            return None\n    else:\n        return None\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string"}}, "process_queue": {"function_name": "process_queue", "import_string": null, "function_string": "\ndef process_queue(node):\n    if not user_queues.empty():\n        message_data = user_queues.get()  # Get the oldest message in the queue\n        if message_data:\n            print(\"Message data: \", message_data)\n            return {\n                \"user_id\" : message_data[\"sender\"], \n                \"message\" : message_data[\"text\"],\n                \"image\" : message_data[\"image\"]\n            }\n        else:\n            return None\n    else:\n        return {\n                \"user_id\" : None, \n                \"message\" : None,\n                \"image\" : None\n            }\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string", "message": "string", "image": "image"}}, "emit_back_to_client": {"function_name": "emit_back_to_client", "import_string": null, "function_string": "\ndef emit_back_to_client(node, user_id=None, message=None):\n    sid = clients.get(user_id)\n    image_url = \"nil\"\n    response_data = {\n        'text': message,\n        'image': image_url,\n        'sender': user_id\n    }\n    if sid:\n        socketio.emit('server_response', response_data, room=sid)\n    else:\n        print(f\"Client {user_id} is not connected\")\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "message": "string"}, "outputs": {"user_id": "string"}}, "stt": {"function_name": "stt", "import_string": null, "function_string": "\nimport os\nimport sys\nimport wave\nimport json\nimport numpy as np\n\nimport argparse\nimport queue\nimport sys\nimport sounddevice as sd\nfrom pydub.silence import split_on_silence\nfrom pydub import AudioSegment\n\nfrom vosk import Model, KaldiRecognizer, SpkModel\n\n\nimport io\nfrom faster_whisper import WhisperModel\n\nimport concurrent.futures\n\ndef int_or_str(text):\n    '''Helper function for argument parsing.'''\n    try:\n        return int(text)\n    except ValueError:\n        return text\n\ndef callback(indata, frames, time, status):\n    '''This is called (from a separate thread) for each audio block.'''\n    if status:\n        print(status, file=sys.stderr)\n    q.put(bytes(indata)) \n\nSPK_MODEL_PATH = 'vosk-model-spk-0.4'\n\nif not os.path.exists(SPK_MODEL_PATH):\n    print('Please download the speaker model from '\n        'https://alphacephei.com/vosk/models and unpack as {SPK_MODEL_PATH} '\n        'in the current folder.')\n    sys.exit(1)\nq = queue.Queue()\n# Large vocabulary free form recognition\nmodel = Model('vosk-model-small-en-us-0.15')\nspk_model = SpkModel(SPK_MODEL_PATH)\n\nmodel_size = 'small'\n# Run on GPU with FP16\nmodel_whisper = WhisperModel(model_size, device='cpu', compute_type='int8')\ndef wav_to_text(audio_path):\n    timeout = 7\n    def transcribe():\n        segments, info = model_whisper.transcribe(audio_path, beam_size=5, word_timestamps=True)\n\n        print('Detected language %s with probability %f', (info.language, info.language_probability))\n        \n        if info.language == 'en':\n            transcribed_text = ''\n            for segment in segments:\n                for word in segment.words:\n                    transcribed_text += word.word + ' '\n                transcribed_text += ' '\n                print(segment.words)\n            print(transcribed_text)\n            return transcribed_text\n\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        future = executor.submit(transcribe)\n        try:\n            return future.result(timeout=timeout)\n        except concurrent.futures.TimeoutError:\n            print('The transcription process timed out.')\n            return None\n\n            \n\n# We compare speakers with cosine distance.\n# We can keep one or several fingerprints for the speaker in a database\n# to distingusih among users.\nspk_sig = [-0.645543, 1.267236, 1.739462, -0.717491, -0.157087, 0.147635, -1.308505, -0.446466, 0.116764, -0.115046, 0.376392, 0.62511, 0.554749, 0.871882, 1.705446, 1.346732, -0.237086, 0.554086, 0.171249, 0.035732, 0.079214, -0.577399, 1.605019, -0.872605, -0.80465, -0.402827, -0.621014, -0.13613, 1.766777, 1.253641, -1.048572, -1.723634, -0.525028, -0.512419, 0.979154, -0.29935, -1.11108, 1.460288, -0.492389, -0.165662, -0.274988, 0.458642, 1.453099, 1.092062, -0.856726, 0.724769, 0.423962, -0.774903, -0.434743, -0.083244, 0.685712, -0.579763, -0.160493, 0.699621, -0.95782, -1.056444, -0.218858, 0.508616, -0.441598, 0.140081, 0.870923, -1.356405, -0.179892, -0.495612, -0.165929, 0.162548, -0.490384, 0.044856, -0.585081, 2.214094, 0.511557, -2.132176, -0.329827, 1.419002, -1.156591, -0.265651, -1.553596, -0.50643, 0.627002, -1.194909, -0.253832, 0.115579, 0.164481, -0.543525, -0.657609, 0.529603, 0.917261, 1.276905, 2.072457, 0.501246, -0.229274, 0.554694, -1.703213, -0.693821, 0.768317, -0.404479, 2.06889, -1.26462, -0.019318, 0.715243, 1.138082, -1.728924, -0.714421, -1.267921, 1.681902, -1.716266, -0.074632, -2.936986, -2.350122, 0.001327, -0.382891, -0.688902, 1.322296, -0.987495, 1.975746, -0.44887, 0.185008, 0.067595, 0.665363, 0.246385, 0.719629, 0.506032, -0.988654, 0.606328, -1.949532, 1.727559, -1.032074, -0.772542]\n\ndef cosine_similarity_average(speaker_embeddings, target_speaker):\n    lowest_similarity = {}\n\n    for speaker, embeddings in speaker_embeddings.items():\n        # Get the lowest similarity among the two embeddings for each speaker\n        similarities = [cosine_dist(target_speaker, embedding) for embedding in embeddings]\n        lowest_similarity[speaker] = min(similarities)\n\n    return lowest_similarity\n\ndef recognize_speaker(target_speaker):\n    speakers = {\n        'speaker1': [[-0.645543, 1.267236, 1.739462, -0.717491, -0.157087, 0.147635, -1.308505, -0.446466, 0.116764, -0.115046, 0.376392, 0.62511, 0.554749, 0.871882, 1.705446, 1.346732, -0.237086, 0.554086, 0.171249, 0.035732, 0.079214, -0.577399, 1.605019, -0.872605, -0.80465, -0.402827, -0.621014, -0.13613, 1.766777, 1.253641, -1.048572, -1.723634, -0.525028, -0.512419, 0.979154, -0.29935, -1.11108, 1.460288, -0.492389, -0.165662, -0.274988, 0.458642, 1.453099, 1.092062, -0.856726, 0.724769, 0.423962, -0.774903, -0.434743, -0.083244, 0.685712, -0.579763, -0.160493, 0.699621, -0.95782, -1.056444, -0.218858, 0.508616, -0.441598, 0.140081, 0.870923, -1.356405, -0.179892, -0.495612, -0.165929, 0.162548, -0.490384, 0.044856, -0.585081, 2.214094, 0.511557, -2.132176, -0.329827, 1.419002, -1.156591, -0.265651, -1.553596, -0.50643, 0.627002, -1.194909, -0.253832, 0.115579, 0.164481, -0.543525, -0.657609, 0.529603, 0.917261, 1.276905, 2.072457, 0.501246, -0.229274, 0.554694, -1.703213, -0.693821, 0.768317, -0.404479, 2.06889, -1.26462, -0.019318, 0.715243, 1.138082, -1.728924, -0.714421, -1.267921, 1.681902, -1.716266, -0.074632, -2.936986, -2.350122, 0.001327, -0.382891, -0.688902, 1.322296, -0.987495, 1.975746, -0.44887, 0.185008, 0.067595, 0.665363, 0.246385, 0.719629, 0.506032, -0.988654, 0.606328, -1.949532, 1.727559, -1.032074, -0.772542],\n                [-0.683516, 0.722179, 1.651159, -0.311776, -0.35272, -0.542711, -0.169784, 0.146419, 0.639174, 0.260786, 0.512685, -0.567375, 0.510885, 1.081993, 0.730045, 1.644301, -0.388575, 0.594761, 0.580934, 1.701163, 0.542753, -0.030902, 0.940672, -0.681181, -0.961269, -0.953732, 0.342842, 0.212761, 1.010038, 0.789226, -0.440633, -1.639356, 0.098124, -0.453873, -0.1269, -0.831008, -1.336311, 1.838328, -1.500506, 0.398561, -0.139225, 0.602066, 1.217693, -0.28669, -1.240536, 0.828214, -0.385781, -1.585939, -0.253948, 0.6254, -1.144157, -1.09649, -1.247936, -0.164992, -1.131125, -0.827816, 1.595752, 1.22196, -0.260766, -0.053225, 0.372862, -0.496685, 0.559101, 0.313831, 0.906749, -0.911119, -0.718342, 0.731359, -0.060828, 0.889468, 0.870002, -1.046849, 0.358473, 1.403957, -0.55995, 0.544278, 0.252579, 0.176449, -0.973618, -1.316356, -1.39273, -0.397281, -1.244906, -2.552846, -0.056479, 0.00252, -0.071661, 0.549343, -0.563582, 0.298601, -1.599536, 0.060805, -1.131684, -0.236406, 0.10192, -0.05143, 2.822287, 0.298605, 0.027687, 1.805171, 0.535367, -0.750344, 0.195215, -2.74342, -0.240448, -1.853602, 0.667115, -1.152912, -1.458451, -0.463823, -1.081316, 1.07476, 1.69582, 0.083853, 0.208222, -0.203687, -0.761975, 2.021879, 2.07578, 0.214109, 1.010975, -0.535104, -1.102454, 1.422523, -1.389488, 2.282245, 0.526214, -0.289677],\n                [-0.645543, 1.267236, 1.739462, -0.717491, -0.157087, 0.147635, -1.308505, -0.446466, 0.116764, -0.115046, 0.376392, 0.62511, 0.554749, 0.871882, 1.705446, 1.346732, -0.237086, 0.554086, 0.171249, 0.035732, 0.079214, -0.577399, 1.605019, -0.872605, -0.80465, -0.402827, -0.621014, -0.13613, 1.766777, 1.253641, -1.048572, -1.723634, -0.525028, -0.512419, 0.979154, -0.29935, -1.11108, 1.460288, -0.492389, -0.165662, -0.274988, 0.458642, 1.453099, 1.092062, -0.856726, 0.724769, 0.423962, -0.774903, -0.434743, -0.083244, 0.685712, -0.579763, -0.160493, 0.699621, -0.95782, -1.056444, -0.218858, 0.508616, -0.441598, 0.140081, 0.870923, -1.356405, -0.179892, -0.495612, -0.165929, 0.162548, -0.490384, 0.044856, -0.585081, 2.214094, 0.511557, -2.132176, -0.329827, 1.419002, -1.156591, -0.265651, -1.553596, -0.50643, 0.627002, -1.194909, -0.253832, 0.115579, 0.164481, -0.543525, -0.657609, 0.529603, 0.917261, 1.276905, 2.072457, 0.501246, -0.229274, 0.554694, -1.703213, -0.693821, 0.768317, -0.404479, 2.06889, -1.26462, -0.019318, 0.715243, 1.138082, -1.728924, -0.714421, -1.267921, 1.681902, -1.716266, -0.074632, -2.936986, -2.350122, 0.001327, -0.382891, -0.688902, 1.322296, -0.987495, 1.975746, -0.44887, 0.185008, 0.067595, 0.665363, 0.246385, 0.719629, 0.506032, -0.988654, 0.606328, -1.949532, 1.727559, -1.032074, -0.772542],\n            \n        ],\n    }\n    #Iterate in speakers and get average cosine similarity and return average cosine similarity for each and the lowest\n    lowest_similarity = cosine_similarity_average(speakers, target_speaker)\n\n    # Print the results\n    print('Lowest Similarities:')\n    dspeaker, thesim = None, None\n    for speaker, lowest_sim in lowest_similarity.items():\n        print(f'{speaker}: {lowest_sim}')\n        dspeaker, thesim = speaker, lowest_sim\n        break\n    \n    #print(cosine_similarity_average(speakers, target_speaker))\n    if thesim > 0.55:\n        dspeaker = 'Unknown User'\n    return dspeaker\ndef cosine_dist(x, y):\n    nx = np.array(x)\n    ny = np.array(y)\n    return 1 - np.dot(nx, ny) / np.linalg.norm(nx) / np.linalg.norm(ny)\ndef save_audio(filename, audio_data, samplerate):\n    audio_segment = AudioSegment(\n        data=audio_data, sample_width=2, frame_rate=samplerate, channels=1\n    )\n    audio_segment.export(filename, format='wav')\n# Function to modify already saved audio file\ndef modify_saved_audio(input_filename, output_filename, samplerate):\n    audio_segment = AudioSegment.from_wav(input_filename)\n\n    # Split the audio based on silence\n    # Adjust silence detection parameters as needed\n    segments = split_on_silence(audio_segment, silence_thresh=-40, keep_silence=100)\n\n    # Concatenate non-silent segments\n    concatenated_audio = AudioSegment.silent()\n    for i, segment in enumerate(segments):\n        concatenated_audio += segment\n\n    # Save the concatenated audio\n    concatenated_audio.export(output_filename, format='wav')\n\n#Create toggle button for STT module\n\n#Initialize these in the node\n#Create node that will initialize these and pass through the STT module\ndevice_info = sd.query_devices(None, 'input')\nsamplerate = int(device_info['default_samplerate'])\n\nrec = KaldiRecognizer(model, samplerate)\nrec.SetSpkModel(spk_model)\n\nstream = sd.RawInputStream(samplerate=samplerate, blocksize=8000, device=None,\n                           dtype='int16', channels=1, callback=callback)\nstream.start()\n\nrecording_started = False\naudio_data = b''\n\nwait_time = 0\nspoken = False\n\ndef stt(node):\n    def update_text_input(dt, transcribed_text):\n        app.root.get_screen(\"chatbox\").ids.text_input.text += transcribed_text\n\n    await asyncio.sleep(0.1)\n    global wait_time\n    global spoken\n    wait_time += 0.1\n    #Connect this and return \n    data = q.get()\n    transcribed_text = ''\n    global recording_started\n    global audio_data\n    \n    app = MDApp.get_running_app()\n    if rec.PartialResult() != '':\n        if not recording_started:\n            print('Recording started.')\n            recording_started = True\n\n    if recording_started:\n        audio_data += data\n\n    if rec.AcceptWaveform(data):\n        res = json.loads(rec.Result())\n        print('Text:', res['text'])\n        print('Recording stopped.')\n        result = res.get('text')\n        print(result)\n        # self.change_text_input(result)\n        save_audio('output_audio.wav', audio_data, samplerate)\n        \n        # After recording is done and you have an output audio file\n        input_audio_filename = 'output_audio.wav'\n        \n        \n        if result:\n            transcribed_text = wav_to_text('output_audio.wav')\n            # Schedule the update after 0 seconds (or you can specify a delay)\n            Clock.schedule_once(lambda dt: update_text_input(dt, transcribed_text))\n            wait_time = 0\n            spoken = True\n        #Connect this and return text\n        \n        #Connect and \n        recording_started = False\n        audio_data = b''\n        if 'spk' in res:\n            recognize_speaker(res['spk'])\n            print('X-vector:', res['spk'])\n    print(transcribed_text)\n    return {'transcribed_text' : transcribed_text}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string", "transcribed_text": "string"}}, "text_to_wav_instance": {"function_name": "text_to_wav_instance", "import_string": null, "function_string": "\n'''\ndef text_to_wav_instance(node, text):\n    return None\n'''\n\nimport time\nimport wave\n\ntts = None\ntry:\n    from TTS.api import TTS\n    tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\nexcept:\n    pass\ndef split_long_sentence(sentence, max_length=250):\n    if len(sentence) <= max_length:\n        return [sentence]\n    \n    sentences = []\n    while len(sentence) > max_length:\n        # Find the last space before max_length\n        last_space_idx = sentence.rfind(' ', 0, max_length)\n        # If no space is found, split at max_length\n        if last_space_idx == -1:\n            last_space_idx = max_length\n        sentences.append(sentence[:last_space_idx])\n        sentence = sentence[last_space_idx+1:]\n    \n    if sentence:\n        sentences.append(sentence)\n    \n    return sentences\ndef get_wav_duration(file_path):\n    with wave.open(file_path, 'rb') as wav_file:\n        # Get the number of frames in the file\n        frames = wav_file.getnframes()\n        # Get the frame rate (number of frames per second)\n        frame_rate = wav_file.getframerate()\n        # Calculate the duration of the file in seconds\n        duration = frames / float(frame_rate)\n        return duration\nengine = pyttsx3.init()\n# Set the rate and volume (optional)\nengine.setProperty('rate', 150)  # Speed of speech\nengine.setProperty('volume', 1.0)  # Volume (0.0 to 1.0)\ndef text_to_wav_instance(node, text):\n    filename = \"output.wav\"\n    global engine\n    if tts:\n        if node.trigger_in.startswith(\"prompt\"):\n            #Split audio first\n            # Tokenize the text into sentences\n            sentences = sent_tokenize(text)\n            split_sentences = []\n            for sentence in sentences:\n                if len(sentence) > 250:\n                    split_sentences.extend(split_long_sentence(sentence))\n                else:\n                    split_sentences.append(sentence)\n            node.args[\"sentences\"] = split_sentences\n            \n            \n        print(node.args[\"sentences\"])\n        \n        while not node.args[\"sentences\"][0]:\n            await asyncio.sleep(0.1)\n        \n\n        \n        if node.args[\"sentences\"][0]:\n            # generate speech by cloning a voice using default settings\n            tts.tts_to_file(node.args[\"sentences\"][0],\n            file_path=\"output.wav\",\n            speaker_wav=\"audio.wav\",\n            speed=1,\n            language=\"en\")\n        else:\n            pass\n        \n        try:\n            node.args[\"sentences\"].pop(0)\n        except:\n            pass\n        \n        try:\n            while node.args[\"sound\"].is_playing():\n                await asyncio.sleep(0.1)\n                print(\"Playing\")\n        except Exception as e:\n            print(\"Error, no sound found\", e)\n            node.args[\"sound\"] = None\n        \n        sound = SoundLoader.load(filename)\n        duration = get_wav_duration(\"output.wav\")\n        \n        \n        return {\"speech_wav\" : sound, \"duration\" : duration}\n    else:\n        engine.save_to_file(text, filename)\n        engine.runAndWait()\n        \n        sound = SoundLoader.load(filename)\n        node.args[\"sound\"] = sound\n        return {\"speech_wav\" : sound}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "text": "string"}, "outputs": {"user_id": "string", "speech_wav": "sound", "duration": "num"}}, "play_audio_tts": {"function_name": "play_audio_tts", "import_string": null, "function_string": "\ndef play_audio_tts(node, sound=None, duration=None):\n    '''\n    if node.trigger_in.startswith(\"text_to_wav_instance\"):\n        if not \"sounds\" in node.args:\n            print(\"Sounds Created\")\n            node.args[\"sounds\"] = []\n            sound.play()\n        if not \"durations\" in node.args:\n            print(\"Durations Created\")\n            node.args[\"durations\"] = []\n        node.args[\"sounds\"].append(sound)\n        node.args[\"durations\"].append(duration)\n        \n    if node.trigger_in.startswith(\"pass_node\"):\n        if node.args[\"sounds\"]:\n            node.args[\"sounds\"][0].play()\n            await asyncio.sleep(node.args[\"durations\"][0])\n            \n            node.args[\"sounds\"].pop(0)\n            node.args[\"durations\"].pop(0)\n            \n            await asyncio.sleep(2)\n            #Delay by audio duration\n    '''\n    if sound:\n        sound.play()\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "sound": "sound", "duration": "num"}, "outputs": {"user_id": "string"}}, "stop_audio_tts": {"function_name": "stop_audio_tts", "import_string": null, "function_string": "\ndef stop_audio_tts(node, sound=None):\n    try:\n        if sound.state == 'play':\n            sound.stop()\n    except:\n        pass\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "sound": "sound"}, "outputs": {"user_id": "string"}}, "reset_input_box": {"function_name": "reset_input_box", "import_string": null, "function_string": "\n\ndef reset_input_box(node):\n    app = MDApp.get_running_app()\n    def update_text_input(dt):\n        app.root.get_screen(\"chatbox\").ids.text_input.text = ''\n    Clock.schedule_once(lambda dt: update_text_input(dt))\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string"}}, "trigger_after_stt": {"function_name": "trigger_after_stt", "import_string": null, "function_string": "\ndef trigger_after_stt(node, transcribed_text=None):\n    global wait_time\n    global spoken\n    if spoken:\n        print(\"Spoken checkpoint\")\n        if wait_time >= 3:\n            print(\"Wait time is: \", wait_time)\n            wait_time = 0\n            spoken = False\n    else:\n        print(\"Stopped: trigger_after_stt\")\n        node.stop = True\n        return None\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "transcribed_text": "string"}, "outputs": {"user_id": "string"}}, "search_facebook": {"function_name": "search_facebook", "import_string": null, "function_string": "\n\ndef search_facebook(node, user_input=None, instruct_type=None):\n    if instruct_type == 3:\n        # Building the command\n        command = ['python', 'search_facebook.py'] + [user_input]\n        \n        # Using subprocess to run the command and capture the output\n        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        #print(result)\n        result = str(result.stdout)\n        #print(result)\n        # Return the stdout and stderr\n        return {'output': result}\n    else:\n        node.stop = True\n        return None\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "user_input": "string", "instruct_type": "num"}, "outputs": {"user_id": "string", "output": "output"}}, "file_chooser": {"function_name": "file_chooser", "import_string": null, "function_string": "\ndef file_chooser(node):\n    print(node, node.node_id, node.output_args)\n    if node.trigger_in.startswith(\"display_output\"):\n        node.output_args = {\"user_image\" : None}\n        return {\"user_image\" : None}\n    else:\n        root = Tk()\n        root.withdraw()\n        file_path = filedialog.askopenfilename()\n        root.destroy()\n        def pop(dt):\n            popup = Popup(title='No file selected',\n                          content=Label(text='No file selected.'),\n                          size_hint=(None, None), size=(400, 200))\n            popup.open()\n        if file_path:\n            #self.image.source = file_path\n            return {\"dir\" : file_path}\n        else:\n            Clock.schedule_once(pop)\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string", "dir": "string"}}, "image_chooser": {"function_name": "image_chooser", "import_string": null, "function_string": "\ndef image_chooser(node):\n    print(node, node.node_id, node.output_args)\n    if node.trigger_in.startswith(\"display_output\"):\n        node.output_args = {\"user_image\" : None}\n        return {\"user_image\" : None}\n    else:\n        root = Tk()\n        root.withdraw()\n        file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.png;*.jpg;*.jpeg\")])\n        root.destroy()\n        def pop(dt):\n            popup = Popup(title='No file selected',\n                          content=Label(text='No file selected.'),\n                          size_hint=(None, None), size=(400, 200))\n            popup.open()\n        if file_path:\n            #self.image.source = file_path\n            return {\"user_image\" : file_path}\n        else:\n            Clock.schedule_once(pop)\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string", "user_image": "string"}}, "display_output": {"function_name": "display_output", "import_string": null, "function_string": "\ndef display_output(node, user_input=None, output=None, instruct_type=None, generated_image_path=None, user_image=None):\n    app = MDApp.get_running_app()\n    print(\"Display Output: \", user_input, output)\n    user_text = user_input or \"test\"\n    response = output or \"test\"\n    await asyncio.sleep(.25)\n    def update_ui(dt):\n        user_header_text = '[b]User[/b] [size=12][color=#A9A9A9]{}[/color][/size]'.format(app.current_date)\n        bot_header_text = '[b]Bot[/b] [size=12][color=#A9A9A9]{}[/color][/size]'.format(app.current_date)\n        \n        user_message = user_header_text + '\\n' + user_text\n        bot_message = bot_header_text + '\\n' + response\n        \n        user_custom_component = CustomComponent(img_source=\"images/user_logo.png\", txt=user_message)\n        bot_custom_component = CustomComponent(img_source=\"images/bot_logo.png\", txt=bot_message)\n        \n        grid_layout = app.root.get_screen(\"chatbox\").ids.grid_layout\n        \n        grid_layout.add_widget(user_custom_component)\n        print(\"User Image: \", user_image)\n        if user_image:\n            print(user_image)\n            grid_layout.add_widget(CustomImageComponent(img_source=user_image))\n        grid_layout.add_widget(bot_custom_component)\n        \n        if instruct_type == 2:\n            #image_components.append(CustomImageComponent(img_source=user_image))\n            grid_layout.add_widget(CustomImageComponent(img_source=generated_image_path))\n        \n    # Schedule the update_ui function to run on the main thread\n    Clock.schedule_once(update_ui)\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "user_input": "string", "output": "string", "instruct_type": "num", "generated_image_path": "string", "user_image": "string"}, "outputs": {"user_id": "string"}}, "select_model": {"function_name": "select_model", "import_string": null, "function_string": "\ndef select_model(node):\n    print(\"select_model\")\n    await asyncio.sleep(.25)\n    return None\n            ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string", "model": "string"}}, "user_input": {"function_name": "user_input", "import_string": null, "function_string": "\ndef user_input(node):\n    try:\n        global spoken\n        print(\"Spoken: \", spoken)\n        if not spoken:\n            spoken = False\n            app = MDApp.get_running_app()\n            user_input = app.root.get_screen(\"chatbox\").ids.text_input.text\n            print(\"Printing user_input: \", user_input)\n            return {\"user_input\" : user_input}\n        else:\n            node.stop = True\n    except:\n        app = MDApp.get_running_app()\n        user_input = app.root.get_screen(\"chatbox\").ids.text_input.text\n        print(\"Printing user_input: \", user_input)\n        return {\"user_input\" : user_input}\n    \n            ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string", "user_input": "string"}}, "pass_node": {"function_name": "pass_node", "import_string": null, "function_string": "\ndef pass_node(node):\n    return None\n            ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string"}}, "context": {"function_name": "context", "import_string": null, "function_string": "\ndef context(node):\n    print(\"context\")\n    await asyncio.sleep(.25)\n    return None\n            ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string", "context": "string"}}, "reset_outputs": {"function_name": "reset_outputs", "import_string": null, "function_string": "\ndef reset_outputs(node):\n    return None\n            ", "description": null, "documentation": null, "inputs": {"user_id": "string"}, "outputs": {"user_id": "string"}}, "is_greater_than": {"function_name": "is_greater_than", "import_string": null, "function_string": "\ndef is_greater_than(node, A=None, B=None):\n    return {\"is_greater_than\" : A > B}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "A": "bool", "B": "bool"}, "outputs": {"user_id": "string", "is_greater_than": "bool"}}, "is_less_than": {"function_name": "is_less_than", "import_string": null, "function_string": "\ndef is_less_than(node, A=None, B=None):\n    return {\"is_less_than\" : A < B}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "A": "bool", "B": "bool"}, "outputs": {"user_id": "string", "is_less_than": "bool"}}, "is_equal": {"function_name": "is_equal", "import_string": null, "function_string": "\ndef is_equal(node, A=None, B=None):\n    return {\"is_equal\" : A == B}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "A": "bool", "B": "bool"}, "outputs": {"user_id": "string", "is_equal": "bool"}}, "get_instruct_type_node": {"function_name": "get_instruct_type_node", "import_string": null, "function_string": "\ndef get_instruct_type_node(node, user_id=None, user_input=None, context=None):\n    # Input text containing the Python code block\n    #print(\"Running get instruct type\")\n    asyncio.sleep(0.1)\n    if user_id == None:\n        node.stop = True\n        return {\"instruct_type\" : 0}\n    if user_input == None:\n        return {\"instruct_type\" : 0}\n    generate_code = (\n        f\"User Input: {user_input}\\n\"\n        \"Instruct Types:\\n\"\n        \"0: Error, if empty user input or just blank spaces, return this\\n\"\n        \"1: Normal, normal conversation\\n\"\n        \"2: Generate Image, if user wants to generate an image\\n\"\n        \"3: Search Facebook, if user wants to search Facebook.\\n\"\n        \"4: Search Google, If user wants to do Web Search or if you dont know the answer or wants updated answer.\\n\"\n        \"5: Search Google with Images, If user wants to search images of an object\\n\"\n        \"First justify why, then output only the number of the instruct type, with format: \\n\"\n        \"Format: instruct type:<number>\"\n    )\n    message_array = []\n    message_array.append({\"role\": \"system\", \"content\": \"Your role is to decide what the instruct type to use based on the user intent. \"})\n    message_array.append({\"role\": \"user\", \"content\": generate_code})\n    if context:\n        message_array.append({\"role\": \"context\", \"content\": context})\n    global TOGETHER_API_KEY\n    client_for_instruct = OpenAI(\n      api_key=TOGETHER_API_KEY,\n      base_url='https://api.together.xyz/v1',\n    )\n    chat_completion = client_for_instruct.chat.completions.create(\n      messages=message_array,\n      model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n    )\n    \n    response = chat_completion.choices[0].message.content\n    print(response)\n    # Use regular expression to find the instruct_type number\n    pattern = re.compile(r\"instruct type\\s*:\\s*(\\d+)\")\n    # Convert the string to all lowercase\n    response = response.lower()\n    match = pattern.search(response)\n    instruct_type = None\n    if match:\n        instruct_type = int(match.group(1))\n        print(f\"instruct_type number: {instruct_type}\")\n    else:\n        print(\"instruct_type not found in the data\")\n    #instruct_type = int(response)\n    print(\"Bot: \", response)\n    return {\"user_id\" : user_id, \"instruct_type\" : instruct_type}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "user_input": "string", "context": "string"}, "outputs": {"user_id": "string", "message": "string", "instruct_type": "num"}}, "generate_image_prompt": {"function_name": "generate_image_prompt", "import_string": null, "function_string": "\ndef generate_image_prompt(node, model=None, user_input=None, context=None, instruct_type=None):\n    app = MDApp.get_running_app()\n    print(\"Prompt\")\n    user_text = user_input\n    if context:\n        context = \"OCR output:\\n\" + context\n        print(\"context: \", context)\n    generated_image_path = \"\"\n    # Continue the conversation            \n    response = app.continue_conversation(user_text=user_text, context=context, user_id=user_id)\n    print(\"output: \", response)\n    return {\"output\" : response, \"generated_image_path\" : generated_image_path}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "model": "string", "user_input": "string", "instruct_type": "num", "context": "string"}, "outputs": {"user_id": "string", "output": "string", "instruct_type": "num"}}, "is_normal_prompt": {"function_name": "is_normal_prompt", "import_string": null, "function_string": "\ndef is_normal_prompt(node, user_id=None, message=None, instruct_type=None):\n    print(\"Printed instruct: \", instruct_type)\n    #if not instruct_type == 1:\n        #node.stop = True\n        #pass\n    print(\"Ran is normal\")\n    return {\"user_id\" : user_id}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "message": "string", "instruct_type": "num"}, "outputs": {"user_id": "string", "message": "string"}}, "prompt": {"function_name": "prompt", "import_string": null, "function_string": "\ndef prompt(node, user_id=None, model=None, user_input=None, context=None, instruct_type=None):\n    app = MDApp.get_running_app()\n    print(\"Ran Prompt\")\n    print(model, user_input, context)\n    user_text = user_input\n    # Continue the conversation\n    if context:\n        context = \"Context: \" + context\n    response = app.continue_conversation(user_text=user_text, context=context, user_id=user_id)\n    print(\"output: \", response)\n    return {\"user_id\" : \"string\", \"output\" : response}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "model": "string", "user_input": "string", "instruct_type": "num", "context": "string"}, "outputs": {"user_id": "string", "output": "string"}}, "image_to_text": {"function_name": "image_to_text", "import_string": null, "function_string": "\ndef image_to_text(node, user_image=None):\n    if user_image:\n        # Load the image using PIL\n        print(user_image)\n        # Convert the image to a format OpenCV can work with\n        image = cv2.imread(user_image)\n        image_cv = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n        # Use pytesseract to get detailed OCR results\n        detailed_data = pytesseract.image_to_data(image_cv, output_type=pytesseract.Output.DICT)\n        \n        # Initialize variables to store sentence/paragraph bounding boxes and text\n        boxes = []\n        current_box = None\n        current_text = \"\"\n\n        # Loop over each of the text elements found in the image\n        for i in range(len(detailed_data['level'])):\n            (x, y, w, h) = (detailed_data['left'][i], detailed_data['top'][i], detailed_data['width'][i], detailed_data['height'][i])\n            text = detailed_data['text'][i]\n            conf = int(detailed_data['conf'][i])\n            \n            # Only consider text elements with a confidence above a certain threshold\n            if conf > 20:\n                if current_box is None:\n                    # Start a new bounding box and text group\n                    current_box = (x, y, x + w, y + h)\n                    current_text = text\n                else:\n                    # Check if the text element is on a new line\n                    if y > current_box[3]:\n                        # Add a newline character\n                        current_text += \"\\n\"\n                    # Expand the current bounding box to include the new text element\n                    current_box = (\n                        min(current_box[0], x),\n                        min(current_box[1], y),\n                        max(current_box[2], x + w),\n                        max(current_box[3], y + h)\n                    )\n                    # Append text to the current group\n                    current_text += \" \" + text\n                \n                # Check if the next element is a new paragraph or sentence (using heuristic)\n                if i == len(detailed_data['level']) - 1 or detailed_data['block_num'][i] != detailed_data['block_num'][i + 1]:\n                    boxes.append((current_box, current_text))\n                    current_box = None\n                    current_text = \"\"\n        output_text = \"\"\n        # Draw bounding boxes around sentences/paragraphs and print the text and bounding box coordinates\n        for ((x1, y1, x2, y2), text) in boxes:\n            #cv2.rectangle(image_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            output_text += f\"Text:\\n{text}\\n\\n\"\n        print(output_text)\n        return {\"output_text\" : output_text}\n    else:\n        return {\"output_text\" : None}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "user_image": "string"}, "outputs": {"user_id": "string", "output_text": "string"}}, "translate_language": {"function_name": "translate_language", "import_string": null, "function_string": "\ndef translate_language(node, input_text=None, input_language=None, output_language=\"English\"):\n    if input_text:\n        if input_language != output_language:\n            global cohere_api_key\n            co = cohere.Client(cohere_api_key) # This is your trial API key\n            fix_prompt = f'Translate each sentence into {output_language}, output only the translation:'\n            response = co.generate(\n                model='c4ai-aya-23',\n                prompt=fix_prompt + input_text,\n                max_tokens=20000,\n                temperature=0.9,\n                k=0,\n                stop_sequences=[],\n                return_likelihoods='NONE')\n            print(\"Translated\", response.generations[0].text)\n            output_text = response.generations[0].text\n            return {\"output_text\" : output_text}\n        else:\n            return {\"output_text\" : input_text}\n    else:\n        return {\"output_text\" : None}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "input_text": "string", "input_language": "string", "output_language": "string"}, "outputs": {"user_id": "string", "output_text": "string"}}, "detect_language": {"function_name": "detect_language", "import_string": null, "function_string": "\ndef detect_language(node, input_text=None):\n    if input_text:\n        try:\n            co = cohere.Client(cohere_api_key) # This is your trial API key\n            # Detect the language of the text\n            # Print the language code and name\n            response = co.generate(\n            model='c4ai-aya-23',\n            prompt=\"Input Text:\\n\" + input_text + \"\\nDetect language, output only ISO 639 language code in format, based on language not content:\\ncode: en,fr,jp,etc.\",\n            max_tokens=30,\n            temperature=0.1,\n            k=0,\n            stop_sequences=[],\n            return_likelihoods='NONE')\n            language_code=response.generations[0].text\n            print(response.generations[0].text)\n            language = language_codes[language_code]\n            print(language_code)\n            print(\"language: \", language)\n            return {\"language\" : language}\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return {\"language\" : \"unknown\"}\n    else:\n        return {\"language\" : \"unknown\"}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "input_text": "string"}, "outputs": {"user_id": "string", "language": "string"}}, "delay": {"function_name": "delay", "import_string": null, "function_string": "\ndef delay(node, delay_seconds=None):\n    if node.trigger_in.startswith(\"time_delta_seconds\"):\n        \n        asyncio.sleep(delay_seconds)\n    elif delay_seconds:\n        asyncio.sleep(delay_seconds)\n    return None\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "delay_seconds": "string"}, "outputs": {"user_id": "string"}}, "time_delta_seconds_from_now": {"function_name": "time_delta_seconds", "import_string": null, "function_string": "\ndef time_delta_seconds(node, given_date_time_str):\n    now = datetime.now()\n    \n    # Given date and time\n    given_date_time = datetime.strptime(given_date_time_str, \"%Y-%m-%d %H:%M:%S\")\n    \n    # Calculate the difference\n    delta = now - given_date_time\n    \n    # Convert the difference to seconds\n    delta_seconds = delta.total_seconds()\n    \n    return {\"seconds\" : delta_seconds}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "given_date_time_str": "string"}, "outputs": {"user_id": "string", "seconds": "num"}}, "decide_output_language": {"function_name": "decide_output_language", "import_string": null, "function_string": "\ndef decide_output_language(node, user_language=None, listener_language=None, user_prompt=None, user_info=None, listener_info=None):\n    if input_text:\n        try:\n            language_code = detect(user_prompt)\n            language = language_codes[language_code]\n            return {\"language\" : language}\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return {\"language\" : \"English\"}\n    else:\n        return {\"language\" : \"English\"}\n        ", "description": null, "documentation": null, "inputs": {"user_id": "string", "user_prompt": "string", "user_language": "string", "user_info": "string", "listener_language": "string", "listener_info": "string"}, "outputs": {"user_id": "string", "language": "string"}}}